# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KTdx48I9nFcGeYpIoUI8sOpWPf_5JN_o
"""

# !pip install tensorflow-determinism

!pip install tensorflow-addons==0.19.0
import tensorflow_addons as tfa

# !pip install tf-nightly

#!/usr/bin/env python
# coding: utf-8
import numpy as np
import datetime
import pandas as pd
import os
seed_value= 42 
####*IMPORANT*: Have to do this line *before* importing tensorflow
os.environ['PYTHONHASHSEED']=str(seed_value)
os.environ["TF_DETERMINISTIC_OPS"] = '1'
os.environ['TF_CUDNN_DETERMINISTIC'] = '1'
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import backend, regularizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import InceptionResNetV2, ResNet50, InceptionV3, DenseNet121
from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization,GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy 
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
import random
from tensorflow.keras.applications.imagenet_utils import preprocess_input
import argparse
from PIL import Image

parser = argparse.ArgumentParser()
parser.add_argument('--gpu_node', type=str, help='specify gpu nodes', default='0')
parser.add_argument('--database', type=str, help='choose RadImageNet or ImageNet', default='RadImageNet')
parser.add_argument('--model_name', type=str, help='choose IRV2/ResNet50/DenseNet121/InceptionV3', default='IRV2')
parser.add_argument('--batch_size', type=int, help='batch size', default=32)
parser.add_argument('--image_size', type=int, help='image size', default=360)
# parser.add_argument('--epoch', type=int, help='number of epochs', default=1)
parser.add_argument('--epoch', type=int, help='number of epochs', default=50)
parser.add_argument('--structure', type=str, help='unfreezeall/freezeall/unfreezetop10', default='unfreezeall')
# parser.add_argument('--learning_rate', type=float, help='learning rate', default=0.0001)
parser.add_argument('--learning_rate', type=float, help='learning rate', default=0.001)
parser.add_argument("--fold",  type=int, help='fold number', default=9)
parser.add_argument("-f", "--file", required=False)


args = parser.parse_args()

def reset_random_seeds():
  # 1. Set `PYTHONHASHSEED` environment variable at a fixed value
  os.environ['PYTHONHASHSEED']=str(seed_value)
  os.environ["TF_DETERMINISTIC_OPS"] = '1'

  # 2. Set `python` built-in pseudo-random generator at a fixed value
  random.seed(seed_value)

  # 3. Set `numpy` pseudo-random generator at a fixed value
  np.random.seed(seed_value)

  # 4. Set `tensorflow` pseudo-random generator at a fixed value
  tf.random.set_seed(seed_value)
  tf.keras.utils.set_random_seed(seed_value)

def set_seeds():
    tf.config.experimental.enable_op_determinism()
    os.environ['PYTHONHASHSEED'] = str(seed_value)
    os.environ["TF_DETERMINISTIC_OPS"] = '1'
    random.seed(seed_value)
    tf.random.set_seed(seed_value)
    tf.keras.utils.set_random_seed(seed_value)
    np.random.seed(seed_value)
    

def set_global_determinism():
    set_seeds()

    os.environ['TF_DETERMINISTIC_OPS'] = '1'
    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'

set_global_determinism()
tf.config.threading.set_inter_op_parallelism_threads(1)
tf.config.threading.set_intra_op_parallelism_threads(1)

print(args.database)

from google.colab import drive
drive.mount('/content/gdrive')

# Configuration.
path = os.getcwd()
path = path + "/gdrive/MyDrive/puc/tcc"

print(path)

# os.chdir("/gdrive")
!ls

# Configuration.
# path = os.getcwd()

# D:\Meu Drive\puc\tcc\RadImageNet_models

train_img_folder = path+'/Datasets/360x360-dataset-category-4-split/split-augmentedV2/train' #4
val_img_folder = path+'/Datasets/360x360-dataset-category-4-split/split-augmentedV2/val' #4
print(train_img_folder)

# destroy the current TensorFlow graph and create a new one
backend.clear_session()
set_global_determinism()

current_time = datetime.datetime.now()
current_time_str = str(current_time).replace(":", ".")

### Limit to the first GPU for this model
os.environ["CUDA_VISIBLE_DEVICES"]= args.gpu_node

### Import pre-trained weights from ImageNet or RadImageNet
database = args.database
if not database in ['RadImageNet', 'ImageNet']:
    raise Exception('Pre-trained database not exists. Please choose ImageNet or RadImageNet')
    
if not args.structure in ['unfreezeall', 'freezeall','unfreezetop10']:
    raise Exception('Freeze any layers? Choose to unfreezeall/freezeall/unfreezetop10 layers for the network.')

### Set up training image size, batch size and number of epochs
image_size = args.image_size
batch_size = args.batch_size
num_epoches = args.epoch
fold = args.fold

### Creat model
def get_compiled_model():
    if not args.model_name in ['IRV2', 'ResNet50', 'DenseNet121', 'InceptionV3']:
        raise Exception('Pre-trained network not exists. Please choose IRV2/ResNet50/DenseNet121/InceptionV3 instead')
    else:
        if args.model_name == 'IRV2':
            if database == 'RadImageNet':
                model_dir = path + "/RadImageNet_models/RadImageNet-IRV2_notop.h5"
                base_model = InceptionResNetV2(weights=model_dir, input_shape=(image_size, image_size, 3), include_top=False,pooling='avg')
            else:
                base_model = InceptionResNetV2(weights='imagenet', input_shape=(image_size, image_size, 3),include_top=False,pooling='avg')
        if args.model_name == 'ResNet50':
            if database == 'RadImageNet':
                model_dir = path + "/RadImageNet_models/RadImageNet-ResNet50_notop.h5"
                base_model = ResNet50(weights=model_dir, input_shape=(image_size, image_size, 3), include_top=False,pooling='avg')
            else:
                base_model = ResNet50(weights='imagenet', input_shape=(image_size, image_size, 3), include_top=False,pooling='avg')
        if args.model_name == 'DenseNet121':
            if database == 'RadImageNet':
                model_dir = path + "/RadImageNet_models/RadImageNet-DenseNet121_notop.h5"
                base_model = DenseNet121(weights=model_dir, input_shape=(image_size, image_size, 3), include_top=False,pooling='avg')
            else:
                base_model = DenseNet121(weights='imagenet', input_shape=(image_size, image_size, 3), include_top=False,pooling='avg')
        if args.model_name == 'InceptionV3':
            if database == 'RadImageNet':
                model_dir = path + "/RadImageNet_models/RadImageNet-InceptionV3_notop.h5"
                base_model = InceptionV3(weights=model_dir, input_shape=(image_size, image_size, 3), include_top=False,pooling='avg') 
            else:
                base_model = InceptionV3(weights='imagenet', input_shape=(image_size, image_size, 3), include_top=False,pooling='avg')
    if args.structure == 'freezeall':
        for layer in base_model.layers:
            layer.trainable = False
    if args.structure == 'unfreezeall':
        pass
    if args.structure == 'unfreezetop10':
        for layer in base_model.layers[:-10]:
            layer.trainable = False
    y = base_model.output

    # creating a dropout layer with a 50% chance of setting inputs to zero
    y = Dropout(0.5, seed=seed_value)(y)

    predictions = Dense(num_classes, activation='softmax')(y)

    model = Model(inputs=base_model.input, outputs=predictions)
    adam = Adam(learning_rate=args.learning_rate)
    model.compile(optimizer=adam, loss=CategoricalCrossentropy(), metrics=['accuracy',
                                                                      tf.keras.metrics.Precision(name='precision'), 
                                                                      tf.keras.metrics.Recall(name='recall'),
                                                                      tfa.metrics.F1Score(num_classes=num_classes,name='f1_score', average='macro')])
    return model
    
def run_model():
    # reset_random_seeds()
    # Call the function with seed value
    set_global_determinism()

    ### Set train steps and validation steps
    model = get_compiled_model()
    train_steps =  len(train_generator.labels)/ batch_size
    val_steps = len(validation_generator.labels) / batch_size
    
    #### set the path to save models having lowest validation loss during training
    save_model_dir = path + '/models/'
    if not os.path.exists(save_model_dir):
        os.mkdir(save_model_dir) 
    filepath= path + "/models/"+ current_time_str +"-v2-binary-thyroid-sigmoid-" + args.structure + "-" + database + "-" + args.model_name + "-" + str(image_size) + "-" + str(batch_size) + "-"+str(args.learning_rate)+"-"+str(num_epoches)+ ".h5"   
  

    earlyStop = EarlyStopping(monitor='val_f1_score', mode='max', verbose=1, patience=60)

    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,
                              patience=5, min_lr=0.000001, verbose=1)
    checkpoint = ModelCheckpoint(filepath, monitor='val_f1_score', verbose=1, save_best_only=True, mode='max')


    set_global_determinism()
    history = model.fit(
            train_generator,
            epochs=num_epoches,
            steps_per_epoch=train_steps,
            validation_data=validation_generator,
            validation_steps=val_steps,
            use_multiprocessing=False,
            callbacks=[checkpoint, earlyStop])
    
    ### Save training loss
    train_accuracy = history.history['accuracy']
    val_accuracy = history.history['val_accuracy']
    train_precision = history.history['precision']
    val_precision = history.history['val_precision']
    train_recall = history.history['recall']
    val_recall = history.history['val_recall']
    train_f1_score = history.history['f1_score']
    val_f1_score = history.history['val_f1_score']
    train_loss = history.history['loss'] 
    val_loss = history.history['val_loss']
    d_loss = pd.DataFrame({'train_accuracy':train_accuracy, 'val_accuracy':val_accuracy, 'train_loss':train_loss, 'val_loss':val_loss,
                           'train_precision':train_precision, 'val_precision':val_precision, 'train_recall':train_recall, 'val_recall':val_recall,
                           'train_f1_score':train_f1_score, 'val_f1_score':val_f1_score})

    save_loss_dir = path + '/loss'
    if not os.path.exists(save_loss_dir):
        os.mkdir(save_loss_dir)
    d_loss.to_csv(path + "/loss/"+ current_time_str +"-v2-binary-thyroid-sigmoid-" + 
                  args.structure + "-" + database + "-" +  args.model_name + "-" +
                  str(image_size) + "-" + str(batch_size) + "-"+str(args.learning_rate)+"-"+
                  str(num_epoches)+ ".csv", index=False)  
    

df_train=pd.read_csv(path + "/csv/train-fold-"+str(fold)+".csv")
df_val=pd.read_csv(path + "/csv/test-fold-"+str(fold)+".csv")

data_generator = ImageDataGenerator(
    rescale = 1/255.)

train_generator = data_generator.flow_from_dataframe(
    dataframe=df_train,
    x_col = 'img_dir',
    y_col = 'label',
    target_size=(image_size, image_size),
    batch_size=batch_size,
    shuffle=True,
    seed=seed_value)

validation_generator = data_generator.flow_from_dataframe(
    dataframe=df_val,
    x_col = 'img_dir',
    y_col = 'label',
    target_size=(image_size, image_size),
    batch_size=batch_size,
    shuffle=True,
    seed=seed_value)


num_classes =len(train_generator.class_indices)
run_model()