{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhRP-kluH7on",
        "outputId": "09e9ee04-236a-4833-818c-3e8760833d26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons==0.19.0\n",
            "  Downloading tensorflow_addons-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons==0.19.0) (23.1)\n",
            "Collecting typeguard>=2.7 (from tensorflow-addons==0.19.0)\n",
            "  Downloading typeguard-4.0.0-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from typeguard>=2.7->tensorflow-addons==0.19.0) (4.5.0)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.19.0 typeguard-4.0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.12.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# !pip install tensorflow-determinism\n",
        "\n",
        "!pip install tensorflow-addons==0.19.0\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "# !pip install tf-nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxY0cSH_MX7a"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "import numpy as np\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import os\n",
        "seed_value= 42 \n",
        "####*IMPORANT*: Have to do this line *before* importing tensorflow\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "os.environ[\"TF_DETERMINISTIC_OPS\"] = '1'\n",
        "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend, regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import InceptionResNetV2, ResNet50, InceptionV3, DenseNet121\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization,GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy \n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
        "import argparse\n",
        "from PIL import Image\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--gpu_node', type=str, help='specify gpu nodes', default='0')\n",
        "parser.add_argument('--database', type=str, help='choose RadImageNet or ImageNet', default='RadImageNet')\n",
        "parser.add_argument('--model_name', type=str, help='choose IRV2/ResNet50/DenseNet121/InceptionV3', default='IRV2')\n",
        "parser.add_argument('--batch_size', type=int, help='batch size', default=32)\n",
        "parser.add_argument('--image_size', type=int, help='image size', default=360)\n",
        "# parser.add_argument('--epoch', type=int, help='number of epochs', default=1)\n",
        "parser.add_argument('--epoch', type=int, help='number of epochs', default=50)\n",
        "parser.add_argument('--structure', type=str, help='unfreezeall/freezeall/unfreezetop10', default='unfreezeall')\n",
        "# parser.add_argument('--learning_rate', type=float, help='learning rate', default=0.0001)\n",
        "parser.add_argument('--learning_rate', type=float, help='learning rate', default=0.001)\n",
        "parser.add_argument(\"--fold\",  type=int, help='fold number', default=9)\n",
        "parser.add_argument(\"-f\", \"--file\", required=False)\n",
        "\n",
        "\n",
        "args = parser.parse_args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkNl_0duxKsO"
      },
      "outputs": [],
      "source": [
        "def reset_random_seeds():\n",
        "  # 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "  os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "  os.environ[\"TF_DETERMINISTIC_OPS\"] = '1'\n",
        "\n",
        "  # 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "  random.seed(seed_value)\n",
        "\n",
        "  # 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "  np.random.seed(seed_value)\n",
        "\n",
        "  # 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
        "  tf.random.set_seed(seed_value)\n",
        "  tf.keras.utils.set_random_seed(seed_value)\n",
        "\n",
        "def set_seeds():\n",
        "    tf.config.experimental.enable_op_determinism()\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "    os.environ[\"TF_DETERMINISTIC_OPS\"] = '1'\n",
        "    random.seed(seed_value)\n",
        "    tf.random.set_seed(seed_value)\n",
        "    tf.keras.utils.set_random_seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    \n",
        "\n",
        "def set_global_determinism():\n",
        "    set_seeds()\n",
        "\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "set_global_determinism()\n",
        "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
        "tf.config.threading.set_intra_op_parallelism_threads(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bSZClo1CKC0",
        "outputId": "7bc28ae1-5228-405e-a1c4-6d7be3a7ccce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RadImageNet\n"
          ]
        }
      ],
      "source": [
        "print(args.database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C_y3ZXBQVv9",
        "outputId": "117eabdd-0ef4-4614-f541-60c299017d56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iLkkN70rsGr",
        "outputId": "5d31e46a-a9ed-4e45-dc36-f940bc089966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/puc/tcc\n"
          ]
        }
      ],
      "source": [
        "# Configuration.\n",
        "path = os.getcwd()\n",
        "path = path + \"/gdrive/MyDrive/puc/tcc\"\n",
        "\n",
        "print(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaB8HEH5zPMD",
        "outputId": "157cf84d-294e-427a-bdbe-6baa48c35c51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gdrive\tsample_data\n"
          ]
        }
      ],
      "source": [
        "# os.chdir(\"/gdrive\")\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6x5QQ4xrKV1",
        "outputId": "77401902-3713-488d-dc1c-3159a4a8cf08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/puc/tcc/Datasets/360x360-dataset-category-4-split/split-augmentedV2/train\n"
          ]
        }
      ],
      "source": [
        "# Configuration.\n",
        "# path = os.getcwd()\n",
        "\n",
        "# D:\\Meu Drive\\puc\\tcc\\RadImageNet_models\n",
        "\n",
        "train_img_folder = path+'/Datasets/360x360-dataset-category-4-split/split-augmentedV2/train' #4\n",
        "val_img_folder = path+'/Datasets/360x360-dataset-category-4-split/split-augmentedV2/val' #4\n",
        "print(train_img_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHJ1-RRLNLD2",
        "outputId": "cbdd81b2-3971-425a-8e23-dbc910c1b272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4800 validated image filenames belonging to 3 classes.\n",
            "Found 560 validated image filenames belonging to 3 classes.\n",
            "Epoch 1/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.9943 - accuracy: 0.4769 - precision: 0.6330 - recall: 0.2131 - f1_score: 0.4559\n",
            "Epoch 1: val_f1_score improved from -inf to 0.39381, saving model to /content/gdrive/MyDrive/puc/tcc/models/2023-06-08 17.22.01.243054-v2-binary-thyroid-sigmoid-unfreezeall-RadImageNet-IRV2-360-32-0.001-50.h5\n",
            "150/150 [==============================] - 407s 2s/step - loss: 0.9943 - accuracy: 0.4769 - precision: 0.6330 - recall: 0.2131 - f1_score: 0.4559 - val_loss: 1.1834 - val_accuracy: 0.3964 - val_precision: 0.4294 - val_recall: 0.2500 - val_f1_score: 0.3938\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.7400 - accuracy: 0.6596 - precision: 0.7228 - recall: 0.5633 - f1_score: 0.6525\n",
            "Epoch 2: val_f1_score improved from 0.39381 to 0.48087, saving model to /content/gdrive/MyDrive/puc/tcc/models/2023-06-08 17.22.01.243054-v2-binary-thyroid-sigmoid-unfreezeall-RadImageNet-IRV2-360-32-0.001-50.h5\n",
            "150/150 [==============================] - 293s 2s/step - loss: 0.7400 - accuracy: 0.6596 - precision: 0.7228 - recall: 0.5633 - f1_score: 0.6525 - val_loss: 1.3099 - val_accuracy: 0.5143 - val_precision: 0.5111 - val_recall: 0.4929 - val_f1_score: 0.4809\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.4838 - accuracy: 0.8033 - precision: 0.8259 - recall: 0.7698 - f1_score: 0.7978\n",
            "Epoch 3: val_f1_score improved from 0.48087 to 0.49411, saving model to /content/gdrive/MyDrive/puc/tcc/models/2023-06-08 17.22.01.243054-v2-binary-thyroid-sigmoid-unfreezeall-RadImageNet-IRV2-360-32-0.001-50.h5\n",
            "150/150 [==============================] - 293s 2s/step - loss: 0.4838 - accuracy: 0.8033 - precision: 0.8259 - recall: 0.7698 - f1_score: 0.7978 - val_loss: 1.8965 - val_accuracy: 0.5143 - val_precision: 0.5255 - val_recall: 0.4786 - val_f1_score: 0.4941\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.2784 - accuracy: 0.8956 - precision: 0.9038 - recall: 0.8863 - f1_score: 0.8929\n",
            "Epoch 4: val_f1_score improved from 0.49411 to 0.52405, saving model to /content/gdrive/MyDrive/puc/tcc/models/2023-06-08 17.22.01.243054-v2-binary-thyroid-sigmoid-unfreezeall-RadImageNet-IRV2-360-32-0.001-50.h5\n",
            "150/150 [==============================] - 287s 2s/step - loss: 0.2784 - accuracy: 0.8956 - precision: 0.9038 - recall: 0.8863 - f1_score: 0.8929 - val_loss: 1.2385 - val_accuracy: 0.5518 - val_precision: 0.5625 - val_recall: 0.5304 - val_f1_score: 0.5240\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1600 - accuracy: 0.9421 - precision: 0.9470 - recall: 0.9385 - f1_score: 0.9410\n",
            "Epoch 5: val_f1_score did not improve from 0.52405\n",
            "150/150 [==============================] - 267s 2s/step - loss: 0.1600 - accuracy: 0.9421 - precision: 0.9470 - recall: 0.9385 - f1_score: 0.9410 - val_loss: 1.8502 - val_accuracy: 0.4661 - val_precision: 0.4747 - val_recall: 0.4518 - val_f1_score: 0.4408\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9608 - precision: 0.9631 - recall: 0.9583 - f1_score: 0.9598\n",
            "Epoch 6: val_f1_score did not improve from 0.52405\n",
            "150/150 [==============================] - 267s 2s/step - loss: 0.1114 - accuracy: 0.9608 - precision: 0.9631 - recall: 0.9583 - f1_score: 0.9598 - val_loss: 2.6941 - val_accuracy: 0.4768 - val_precision: 0.4772 - val_recall: 0.4679 - val_f1_score: 0.4487\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9742 - precision: 0.9754 - recall: 0.9731 - f1_score: 0.9732\n",
            "Epoch 7: val_f1_score did not improve from 0.52405\n",
            "150/150 [==============================] - 265s 2s/step - loss: 0.0726 - accuracy: 0.9742 - precision: 0.9754 - recall: 0.9731 - f1_score: 0.9732 - val_loss: 1.9296 - val_accuracy: 0.5232 - val_precision: 0.5250 - val_recall: 0.5054 - val_f1_score: 0.5179\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.9840 - precision: 0.9848 - recall: 0.9837 - f1_score: 0.9840\n",
            "Epoch 8: val_f1_score did not improve from 0.52405\n",
            "150/150 [==============================] - 278s 2s/step - loss: 0.0525 - accuracy: 0.9840 - precision: 0.9848 - recall: 0.9837 - f1_score: 0.9840 - val_loss: 3.4170 - val_accuracy: 0.4232 - val_precision: 0.4240 - val_recall: 0.4232 - val_f1_score: 0.4072\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9742 - precision: 0.9749 - recall: 0.9729 - f1_score: 0.9734\n",
            "Epoch 9: val_f1_score did not improve from 0.52405\n",
            "150/150 [==============================] - 267s 2s/step - loss: 0.0741 - accuracy: 0.9742 - precision: 0.9749 - recall: 0.9729 - f1_score: 0.9734 - val_loss: 3.6233 - val_accuracy: 0.4161 - val_precision: 0.4153 - val_recall: 0.3982 - val_f1_score: 0.4080\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0523 - accuracy: 0.9833 - precision: 0.9835 - recall: 0.9827 - f1_score: 0.9831\n",
            "Epoch 10: val_f1_score did not improve from 0.52405\n",
            "150/150 [==============================] - 266s 2s/step - loss: 0.0523 - accuracy: 0.9833 - precision: 0.9835 - recall: 0.9827 - f1_score: 0.9831 - val_loss: 3.7059 - val_accuracy: 0.5089 - val_precision: 0.5117 - val_recall: 0.5089 - val_f1_score: 0.4947\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9781 - precision: 0.9793 - recall: 0.9781 - f1_score: 0.9772\n",
            "Epoch 11: val_f1_score did not improve from 0.52405\n",
            "150/150 [==============================] - 279s 2s/step - loss: 0.0631 - accuracy: 0.9781 - precision: 0.9793 - recall: 0.9781 - f1_score: 0.9772 - val_loss: 4.3910 - val_accuracy: 0.3839 - val_precision: 0.3870 - val_recall: 0.3821 - val_f1_score: 0.3677\n",
            "Epoch 12/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9837 - precision: 0.9846 - recall: 0.9827 - f1_score: 0.9833\n",
            "Epoch 12: val_f1_score did not improve from 0.52405\n",
            "150/150 [==============================] - 267s 2s/step - loss: 0.0503 - accuracy: 0.9837 - precision: 0.9846 - recall: 0.9827 - f1_score: 0.9833 - val_loss: 2.8559 - val_accuracy: 0.4339 - val_precision: 0.4376 - val_recall: 0.4321 - val_f1_score: 0.3902\n",
            "Epoch 13/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9967 - precision: 0.9967 - recall: 0.9967 - f1_score: 0.9967\n",
            "Epoch 13: val_f1_score did not improve from 0.52405\n",
            "150/150 [==============================] - 279s 2s/step - loss: 0.0166 - accuracy: 0.9967 - precision: 0.9967 - recall: 0.9967 - f1_score: 0.9967 - val_loss: 3.1715 - val_accuracy: 0.4625 - val_precision: 0.4647 - val_recall: 0.4589 - val_f1_score: 0.4313\n",
            "Epoch 14/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9960 - precision: 0.9960 - recall: 0.9958 - f1_score: 0.9960\n",
            "Epoch 14: val_f1_score did not improve from 0.52405\n",
            "150/150 [==============================] - 266s 2s/step - loss: 0.0118 - accuracy: 0.9960 - precision: 0.9960 - recall: 0.9958 - f1_score: 0.9960 - val_loss: 3.2593 - val_accuracy: 0.5089 - val_precision: 0.5072 - val_recall: 0.5036 - val_f1_score: 0.5066\n",
            "Epoch 15/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9773 - precision: 0.9777 - recall: 0.9767 - f1_score: 0.9770\n",
            "Epoch 15: val_f1_score improved from 0.52405 to 0.54410, saving model to /content/gdrive/MyDrive/puc/tcc/models/2023-06-08 17.22.01.243054-v2-binary-thyroid-sigmoid-unfreezeall-RadImageNet-IRV2-360-32-0.001-50.h5\n",
            "150/150 [==============================] - 284s 2s/step - loss: 0.0743 - accuracy: 0.9773 - precision: 0.9777 - recall: 0.9767 - f1_score: 0.9770 - val_loss: 1.9919 - val_accuracy: 0.5429 - val_precision: 0.5523 - val_recall: 0.5375 - val_f1_score: 0.5441\n",
            "Epoch 16/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9787 - precision: 0.9789 - recall: 0.9785 - f1_score: 0.9778\n",
            "Epoch 16: val_f1_score did not improve from 0.54410\n",
            "150/150 [==============================] - 270s 2s/step - loss: 0.0652 - accuracy: 0.9787 - precision: 0.9789 - recall: 0.9785 - f1_score: 0.9778 - val_loss: 4.1836 - val_accuracy: 0.4375 - val_precision: 0.4375 - val_recall: 0.4375 - val_f1_score: 0.4050\n",
            "Epoch 17/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9923 - precision: 0.9925 - recall: 0.9923 - f1_score: 0.9921\n",
            "Epoch 17: val_f1_score did not improve from 0.54410\n",
            "150/150 [==============================] - 268s 2s/step - loss: 0.0251 - accuracy: 0.9923 - precision: 0.9925 - recall: 0.9923 - f1_score: 0.9921 - val_loss: 5.1008 - val_accuracy: 0.4554 - val_precision: 0.4562 - val_recall: 0.4554 - val_f1_score: 0.4235\n",
            "Epoch 18/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9865 - precision: 0.9869 - recall: 0.9862 - f1_score: 0.9863\n",
            "Epoch 18: val_f1_score did not improve from 0.54410\n",
            "150/150 [==============================] - 278s 2s/step - loss: 0.0389 - accuracy: 0.9865 - precision: 0.9869 - recall: 0.9862 - f1_score: 0.9863 - val_loss: 5.1456 - val_accuracy: 0.4036 - val_precision: 0.4051 - val_recall: 0.4000 - val_f1_score: 0.3529\n",
            "Epoch 19/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9954 - precision: 0.9956 - recall: 0.9954 - f1_score: 0.9952\n",
            "Epoch 19: val_f1_score did not improve from 0.54410\n",
            "150/150 [==============================] - 267s 2s/step - loss: 0.0159 - accuracy: 0.9954 - precision: 0.9956 - recall: 0.9954 - f1_score: 0.9952 - val_loss: 2.3964 - val_accuracy: 0.5250 - val_precision: 0.5281 - val_recall: 0.5196 - val_f1_score: 0.5204\n",
            "Epoch 20/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998 - precision: 0.9998 - recall: 0.9998 - f1_score: 0.9998\n",
            "Epoch 20: val_f1_score did not improve from 0.54410\n",
            "150/150 [==============================] - 272s 2s/step - loss: 0.0017 - accuracy: 0.9998 - precision: 0.9998 - recall: 0.9998 - f1_score: 0.9998 - val_loss: 3.0798 - val_accuracy: 0.5036 - val_precision: 0.5063 - val_recall: 0.5018 - val_f1_score: 0.4827\n",
            "Epoch 21/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0031 - accuracy: 0.9990 - precision: 0.9990 - recall: 0.9990 - f1_score: 0.9988\n",
            "Epoch 21: val_f1_score did not improve from 0.54410\n",
            "150/150 [==============================] - 275s 2s/step - loss: 0.0031 - accuracy: 0.9990 - precision: 0.9990 - recall: 0.9990 - f1_score: 0.9988 - val_loss: 2.3892 - val_accuracy: 0.5089 - val_precision: 0.5145 - val_recall: 0.5054 - val_f1_score: 0.4940\n",
            "Epoch 22/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9869 - precision: 0.9871 - recall: 0.9867 - f1_score: 0.9863\n",
            "Epoch 22: val_f1_score did not improve from 0.54410\n",
            "150/150 [==============================] - 265s 2s/step - loss: 0.0407 - accuracy: 0.9869 - precision: 0.9871 - recall: 0.9867 - f1_score: 0.9863 - val_loss: 4.6732 - val_accuracy: 0.4411 - val_precision: 0.4442 - val_recall: 0.4411 - val_f1_score: 0.4026\n",
            "Epoch 23/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0994 - accuracy: 0.9690 - precision: 0.9702 - recall: 0.9683 - f1_score: 0.9687\n",
            "Epoch 23: val_f1_score did not improve from 0.54410\n",
            "150/150 [==============================] - 269s 2s/step - loss: 0.0994 - accuracy: 0.9690 - precision: 0.9702 - recall: 0.9683 - f1_score: 0.9687 - val_loss: 2.8552 - val_accuracy: 0.4607 - val_precision: 0.4624 - val_recall: 0.4607 - val_f1_score: 0.4265\n",
            "Epoch 24/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9900 - precision: 0.9900 - recall: 0.9898 - f1_score: 0.9893\n",
            "Epoch 24: val_f1_score did not improve from 0.54410\n",
            "150/150 [==============================] - 262s 2s/step - loss: 0.0310 - accuracy: 0.9900 - precision: 0.9900 - recall: 0.9898 - f1_score: 0.9893 - val_loss: 3.6461 - val_accuracy: 0.4143 - val_precision: 0.4132 - val_recall: 0.4036 - val_f1_score: 0.3962\n",
            "Epoch 25/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9954 - precision: 0.9954 - recall: 0.9954 - f1_score: 0.9951\n",
            "Epoch 25: val_f1_score improved from 0.54410 to 0.55818, saving model to /content/gdrive/MyDrive/puc/tcc/models/2023-06-08 17.22.01.243054-v2-binary-thyroid-sigmoid-unfreezeall-RadImageNet-IRV2-360-32-0.001-50.h5\n",
            "150/150 [==============================] - 281s 2s/step - loss: 0.0162 - accuracy: 0.9954 - precision: 0.9954 - recall: 0.9954 - f1_score: 0.9951 - val_loss: 2.6142 - val_accuracy: 0.5714 - val_precision: 0.5712 - val_recall: 0.5661 - val_f1_score: 0.5582\n",
            "Epoch 26/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9925 - precision: 0.9927 - recall: 0.9923 - f1_score: 0.9924\n",
            "Epoch 26: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 264s 2s/step - loss: 0.0206 - accuracy: 0.9925 - precision: 0.9927 - recall: 0.9923 - f1_score: 0.9924 - val_loss: 2.5576 - val_accuracy: 0.5143 - val_precision: 0.5180 - val_recall: 0.5143 - val_f1_score: 0.4983\n",
            "Epoch 27/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0060 - accuracy: 0.9983 - precision: 0.9983 - recall: 0.9983 - f1_score: 0.9983\n",
            "Epoch 27: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 275s 2s/step - loss: 0.0060 - accuracy: 0.9983 - precision: 0.9983 - recall: 0.9983 - f1_score: 0.9983 - val_loss: 4.0401 - val_accuracy: 0.4643 - val_precision: 0.4632 - val_recall: 0.4607 - val_f1_score: 0.4397\n",
            "Epoch 28/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9883 - precision: 0.9883 - recall: 0.9881 - f1_score: 0.9881\n",
            "Epoch 28: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 265s 2s/step - loss: 0.0342 - accuracy: 0.9883 - precision: 0.9883 - recall: 0.9881 - f1_score: 0.9881 - val_loss: 2.8194 - val_accuracy: 0.5071 - val_precision: 0.5072 - val_recall: 0.5036 - val_f1_score: 0.4727\n",
            "Epoch 29/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9817 - precision: 0.9823 - recall: 0.9812 - f1_score: 0.9812\n",
            "Epoch 29: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 275s 2s/step - loss: 0.0547 - accuracy: 0.9817 - precision: 0.9823 - recall: 0.9812 - f1_score: 0.9812 - val_loss: 2.2924 - val_accuracy: 0.4982 - val_precision: 0.5009 - val_recall: 0.4893 - val_f1_score: 0.4805\n",
            "Epoch 30/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9971 - precision: 0.9971 - recall: 0.9971 - f1_score: 0.9969\n",
            "Epoch 30: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 274s 2s/step - loss: 0.0073 - accuracy: 0.9971 - precision: 0.9971 - recall: 0.9971 - f1_score: 0.9969 - val_loss: 3.5492 - val_accuracy: 0.4143 - val_precision: 0.4150 - val_recall: 0.4143 - val_f1_score: 0.3819\n",
            "Epoch 31/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9942 - precision: 0.9942 - recall: 0.9942 - f1_score: 0.9941\n",
            "Epoch 31: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 263s 2s/step - loss: 0.0186 - accuracy: 0.9942 - precision: 0.9942 - recall: 0.9942 - f1_score: 0.9941 - val_loss: 2.3633 - val_accuracy: 0.4571 - val_precision: 0.4624 - val_recall: 0.4500 - val_f1_score: 0.4571\n",
            "Epoch 32/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9921 - precision: 0.9921 - recall: 0.9919 - f1_score: 0.9918\n",
            "Epoch 32: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 271s 2s/step - loss: 0.0282 - accuracy: 0.9921 - precision: 0.9921 - recall: 0.9919 - f1_score: 0.9918 - val_loss: 3.0633 - val_accuracy: 0.5143 - val_precision: 0.5162 - val_recall: 0.5125 - val_f1_score: 0.4846\n",
            "Epoch 33/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9998 - precision: 0.9998 - recall: 0.9998 - f1_score: 0.9998\n",
            "Epoch 33: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 265s 2s/step - loss: 0.0018 - accuracy: 0.9998 - precision: 0.9998 - recall: 0.9998 - f1_score: 0.9998 - val_loss: 3.0154 - val_accuracy: 0.5036 - val_precision: 0.5054 - val_recall: 0.5000 - val_f1_score: 0.4877\n",
            "Epoch 34/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9962 - precision: 0.9962 - recall: 0.9962 - f1_score: 0.9963\n",
            "Epoch 34: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 269s 2s/step - loss: 0.0125 - accuracy: 0.9962 - precision: 0.9962 - recall: 0.9962 - f1_score: 0.9963 - val_loss: 2.1213 - val_accuracy: 0.5464 - val_precision: 0.5488 - val_recall: 0.5321 - val_f1_score: 0.5418\n",
            "Epoch 35/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9796 - precision: 0.9812 - recall: 0.9792 - f1_score: 0.9792\n",
            "Epoch 35: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 266s 2s/step - loss: 0.0699 - accuracy: 0.9796 - precision: 0.9812 - recall: 0.9792 - f1_score: 0.9792 - val_loss: 1.9652 - val_accuracy: 0.5321 - val_precision: 0.5351 - val_recall: 0.5179 - val_f1_score: 0.5218\n",
            "Epoch 36/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 0.9994 - precision: 0.9994 - recall: 0.9992 - f1_score: 0.9994\n",
            "Epoch 36: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 262s 2s/step - loss: 0.0048 - accuracy: 0.9994 - precision: 0.9994 - recall: 0.9992 - f1_score: 0.9994 - val_loss: 5.2508 - val_accuracy: 0.3857 - val_precision: 0.3846 - val_recall: 0.3839 - val_f1_score: 0.3279\n",
            "Epoch 37/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9921 - precision: 0.9923 - recall: 0.9921 - f1_score: 0.9923\n",
            "Epoch 37: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 274s 2s/step - loss: 0.0234 - accuracy: 0.9921 - precision: 0.9923 - recall: 0.9921 - f1_score: 0.9923 - val_loss: 2.7530 - val_accuracy: 0.4875 - val_precision: 0.4891 - val_recall: 0.4821 - val_f1_score: 0.4831\n",
            "Epoch 38/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9975 - precision: 0.9975 - recall: 0.9975 - f1_score: 0.9973\n",
            "Epoch 38: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 272s 2s/step - loss: 0.0116 - accuracy: 0.9975 - precision: 0.9975 - recall: 0.9975 - f1_score: 0.9973 - val_loss: 3.3931 - val_accuracy: 0.4625 - val_precision: 0.4632 - val_recall: 0.4607 - val_f1_score: 0.4431\n",
            "Epoch 39/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 5.5260e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1_score: 1.0000\n",
            "Epoch 39: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 263s 2s/step - loss: 5.5260e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1_score: 1.0000 - val_loss: 3.3371 - val_accuracy: 0.4429 - val_precision: 0.4427 - val_recall: 0.4411 - val_f1_score: 0.4201\n",
            "Epoch 40/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 2.3742e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1_score: 1.0000\n",
            "Epoch 40: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 275s 2s/step - loss: 2.3742e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1_score: 1.0000 - val_loss: 3.1082 - val_accuracy: 0.4554 - val_precision: 0.4560 - val_recall: 0.4536 - val_f1_score: 0.4381\n",
            "Epoch 41/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 1.2561e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1_score: 1.0000\n",
            "Epoch 41: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 272s 2s/step - loss: 1.2561e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1_score: 1.0000 - val_loss: 3.0851 - val_accuracy: 0.4625 - val_precision: 0.4640 - val_recall: 0.4607 - val_f1_score: 0.4470\n",
            "Epoch 42/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 1.6875e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1_score: 1.0000\n",
            "Epoch 42: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 265s 2s/step - loss: 1.6875e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1_score: 1.0000 - val_loss: 3.2411 - val_accuracy: 0.4625 - val_precision: 0.4665 - val_recall: 0.4607 - val_f1_score: 0.4464\n",
            "Epoch 43/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 1.0496e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1_score: 1.0000\n",
            "Epoch 43: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 266s 2s/step - loss: 1.0496e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1_score: 1.0000 - val_loss: 3.1901 - val_accuracy: 0.4696 - val_precision: 0.4702 - val_recall: 0.4643 - val_f1_score: 0.4559\n",
            "Epoch 44/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 8.8901e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1_score: 1.0000\n",
            "Epoch 44: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 263s 2s/step - loss: 8.8901e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1_score: 1.0000 - val_loss: 3.1707 - val_accuracy: 0.4875 - val_precision: 0.4892 - val_recall: 0.4839 - val_f1_score: 0.4732\n",
            "Epoch 45/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 6.9054e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1_score: 1.0000\n",
            "Epoch 45: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 266s 2s/step - loss: 6.9054e-05 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1_score: 1.0000 - val_loss: 3.2546 - val_accuracy: 0.4661 - val_precision: 0.4668 - val_recall: 0.4643 - val_f1_score: 0.4505\n",
            "Epoch 46/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 0.9840 - precision: 0.9842 - recall: 0.9837 - f1_score: 0.9834\n",
            "Epoch 46: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 262s 2s/step - loss: 0.0508 - accuracy: 0.9840 - precision: 0.9842 - recall: 0.9837 - f1_score: 0.9834 - val_loss: 5.3803 - val_accuracy: 0.3768 - val_precision: 0.3768 - val_recall: 0.3768 - val_f1_score: 0.2775\n",
            "Epoch 47/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.1017 - accuracy: 0.9675 - precision: 0.9695 - recall: 0.9660 - f1_score: 0.9672\n",
            "Epoch 47: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 273s 2s/step - loss: 0.1017 - accuracy: 0.9675 - precision: 0.9695 - recall: 0.9660 - f1_score: 0.9672 - val_loss: 3.5731 - val_accuracy: 0.3893 - val_precision: 0.3945 - val_recall: 0.3839 - val_f1_score: 0.3491\n",
            "Epoch 48/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9852 - precision: 0.9854 - recall: 0.9844 - f1_score: 0.9847\n",
            "Epoch 48: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 272s 2s/step - loss: 0.0413 - accuracy: 0.9852 - precision: 0.9854 - recall: 0.9844 - f1_score: 0.9847 - val_loss: 4.0885 - val_accuracy: 0.3893 - val_precision: 0.3928 - val_recall: 0.3893 - val_f1_score: 0.3463\n",
            "Epoch 49/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9946 - precision: 0.9950 - recall: 0.9942 - f1_score: 0.9945\n",
            "Epoch 49: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 272s 2s/step - loss: 0.0206 - accuracy: 0.9946 - precision: 0.9950 - recall: 0.9942 - f1_score: 0.9945 - val_loss: 1.9854 - val_accuracy: 0.5214 - val_precision: 0.5235 - val_recall: 0.5179 - val_f1_score: 0.5159\n",
            "Epoch 50/50\n",
            "150/150 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998 - precision: 0.9998 - recall: 0.9998 - f1_score: 0.9998\n",
            "Epoch 50: val_f1_score did not improve from 0.55818\n",
            "150/150 [==============================] - 259s 2s/step - loss: 0.0013 - accuracy: 0.9998 - precision: 0.9998 - recall: 0.9998 - f1_score: 0.9998 - val_loss: 2.6730 - val_accuracy: 0.4911 - val_precision: 0.4901 - val_recall: 0.4875 - val_f1_score: 0.4776\n"
          ]
        }
      ],
      "source": [
        "# destroy the current TensorFlow graph and create a new one\n",
        "backend.clear_session()\n",
        "set_global_determinism()\n",
        "\n",
        "current_time = datetime.datetime.now()\n",
        "current_time_str = str(current_time).replace(\":\", \".\")\n",
        "\n",
        "### Limit to the first GPU for this model\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]= args.gpu_node\n",
        "\n",
        "### Import pre-trained weights from ImageNet or RadImageNet\n",
        "database = args.database\n",
        "if not database in ['RadImageNet', 'ImageNet']:\n",
        "    raise Exception('Pre-trained database not exists. Please choose ImageNet or RadImageNet')\n",
        "    \n",
        "if not args.structure in ['unfreezeall', 'freezeall','unfreezetop10']:\n",
        "    raise Exception('Freeze any layers? Choose to unfreezeall/freezeall/unfreezetop10 layers for the network.')\n",
        "\n",
        "### Set up training image size, batch size and number of epochs\n",
        "image_size = args.image_size\n",
        "batch_size = args.batch_size\n",
        "num_epoches = args.epoch\n",
        "fold = args.fold\n",
        "\n",
        "### Creat model\n",
        "def get_compiled_model():\n",
        "    if not args.model_name in ['IRV2', 'ResNet50', 'DenseNet121', 'InceptionV3']:\n",
        "        raise Exception('Pre-trained network not exists. Please choose IRV2/ResNet50/DenseNet121/InceptionV3 instead')\n",
        "    else:\n",
        "        if args.model_name == 'IRV2':\n",
        "            if database == 'RadImageNet':\n",
        "                model_dir = path + \"/RadImageNet_models/RadImageNet-IRV2_notop.h5\"\n",
        "                base_model = InceptionResNetV2(weights=model_dir, input_shape=(image_size, image_size, 3), include_top=False,pooling='avg')\n",
        "            else:\n",
        "                base_model = InceptionResNetV2(weights='imagenet', input_shape=(image_size, image_size, 3),include_top=False,pooling='avg')\n",
        "        if args.model_name == 'ResNet50':\n",
        "            if database == 'RadImageNet':\n",
        "                model_dir = path + \"/RadImageNet_models/RadImageNet-ResNet50_notop.h5\"\n",
        "                base_model = ResNet50(weights=model_dir, input_shape=(image_size, image_size, 3), include_top=False,pooling='avg')\n",
        "            else:\n",
        "                base_model = ResNet50(weights='imagenet', input_shape=(image_size, image_size, 3), include_top=False,pooling='avg')\n",
        "        if args.model_name == 'DenseNet121':\n",
        "            if database == 'RadImageNet':\n",
        "                model_dir = path + \"/RadImageNet_models/RadImageNet-DenseNet121_notop.h5\"\n",
        "                base_model = DenseNet121(weights=model_dir, input_shape=(image_size, image_size, 3), include_top=False,pooling='avg')\n",
        "            else:\n",
        "                base_model = DenseNet121(weights='imagenet', input_shape=(image_size, image_size, 3), include_top=False,pooling='avg')\n",
        "        if args.model_name == 'InceptionV3':\n",
        "            if database == 'RadImageNet':\n",
        "                model_dir = path + \"/RadImageNet_models/RadImageNet-InceptionV3_notop.h5\"\n",
        "                base_model = InceptionV3(weights=model_dir, input_shape=(image_size, image_size, 3), include_top=False,pooling='avg') \n",
        "            else:\n",
        "                base_model = InceptionV3(weights='imagenet', input_shape=(image_size, image_size, 3), include_top=False,pooling='avg')\n",
        "    if args.structure == 'freezeall':\n",
        "        for layer in base_model.layers:\n",
        "            layer.trainable = False\n",
        "    if args.structure == 'unfreezeall':\n",
        "        pass\n",
        "    if args.structure == 'unfreezetop10':\n",
        "        for layer in base_model.layers[:-10]:\n",
        "            layer.trainable = False\n",
        "    y = base_model.output\n",
        "\n",
        "    # creating a dropout layer with a 50% chance of setting inputs to zero\n",
        "    y = Dropout(0.5, seed=seed_value)(y)\n",
        "\n",
        "    predictions = Dense(num_classes, activation='softmax')(y)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    adam = Adam(learning_rate=args.learning_rate)\n",
        "    model.compile(optimizer=adam, loss=CategoricalCrossentropy(), metrics=['accuracy',\n",
        "                                                                      tf.keras.metrics.Precision(name='precision'), \n",
        "                                                                      tf.keras.metrics.Recall(name='recall'),\n",
        "                                                                      tfa.metrics.F1Score(num_classes=num_classes,name='f1_score', average='macro')])\n",
        "    return model\n",
        "    \n",
        "def run_model():\n",
        "    # reset_random_seeds()\n",
        "    # Call the function with seed value\n",
        "    set_global_determinism()\n",
        "\n",
        "    ### Set train steps and validation steps\n",
        "    model = get_compiled_model()\n",
        "    train_steps =  len(train_generator.labels)/ batch_size\n",
        "    val_steps = len(validation_generator.labels) / batch_size\n",
        "    \n",
        "    #### set the path to save models having lowest validation loss during training\n",
        "    save_model_dir = path + '/models/'\n",
        "    if not os.path.exists(save_model_dir):\n",
        "        os.mkdir(save_model_dir) \n",
        "    filepath= path + \"/models/\"+ current_time_str +\"-v2-binary-thyroid-sigmoid-\" + args.structure + \"-\" + database + \"-\" + args.model_name + \"-\" + str(image_size) + \"-\" + str(batch_size) + \"-\"+str(args.learning_rate)+\"-\"+str(num_epoches)+ \".h5\"   \n",
        "  \n",
        "\n",
        "    earlyStop = EarlyStopping(monitor='val_f1_score', mode='max', verbose=1, patience=60)\n",
        "\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
        "                              patience=5, min_lr=0.000001, verbose=1)\n",
        "    checkpoint = ModelCheckpoint(filepath, monitor='val_f1_score', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "    set_global_determinism()\n",
        "    history = model.fit(\n",
        "            train_generator,\n",
        "            epochs=num_epoches,\n",
        "            steps_per_epoch=train_steps,\n",
        "            validation_data=validation_generator,\n",
        "            validation_steps=val_steps,\n",
        "            use_multiprocessing=False,\n",
        "            callbacks=[checkpoint, earlyStop])\n",
        "    \n",
        "    ### Save training loss\n",
        "    train_accuracy = history.history['accuracy']\n",
        "    val_accuracy = history.history['val_accuracy']\n",
        "    train_precision = history.history['precision']\n",
        "    val_precision = history.history['val_precision']\n",
        "    train_recall = history.history['recall']\n",
        "    val_recall = history.history['val_recall']\n",
        "    train_f1_score = history.history['f1_score']\n",
        "    val_f1_score = history.history['val_f1_score']\n",
        "    train_loss = history.history['loss'] \n",
        "    val_loss = history.history['val_loss']\n",
        "    d_loss = pd.DataFrame({'train_accuracy':train_accuracy, 'val_accuracy':val_accuracy, 'train_loss':train_loss, 'val_loss':val_loss,\n",
        "                           'train_precision':train_precision, 'val_precision':val_precision, 'train_recall':train_recall, 'val_recall':val_recall,\n",
        "                           'train_f1_score':train_f1_score, 'val_f1_score':val_f1_score})\n",
        "\n",
        "    save_loss_dir = path + '/loss'\n",
        "    if not os.path.exists(save_loss_dir):\n",
        "        os.mkdir(save_loss_dir)\n",
        "    d_loss.to_csv(path + \"/loss/\"+ current_time_str +\"-v2-binary-thyroid-sigmoid-\" + \n",
        "                  args.structure + \"-\" + database + \"-\" +  args.model_name + \"-\" +\n",
        "                  str(image_size) + \"-\" + str(batch_size) + \"-\"+str(args.learning_rate)+\"-\"+\n",
        "                  str(num_epoches)+ \".csv\", index=False)  \n",
        "    \n",
        "\n",
        "df_train=pd.read_csv(path + \"/csv/train-fold-\"+str(fold)+\".csv\")\n",
        "df_val=pd.read_csv(path + \"/csv/test-fold-\"+str(fold)+\".csv\")\n",
        "\n",
        "data_generator = ImageDataGenerator(\n",
        "    rescale = 1/255.)\n",
        "\n",
        "train_generator = data_generator.flow_from_dataframe(\n",
        "    dataframe=df_train,\n",
        "    x_col = 'img_dir',\n",
        "    y_col = 'label',\n",
        "    target_size=(image_size, image_size),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    seed=seed_value)\n",
        "\n",
        "validation_generator = data_generator.flow_from_dataframe(\n",
        "    dataframe=df_val,\n",
        "    x_col = 'img_dir',\n",
        "    y_col = 'label',\n",
        "    target_size=(image_size, image_size),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    seed=seed_value)\n",
        "\n",
        "\n",
        "num_classes =len(train_generator.class_indices)\n",
        "run_model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}